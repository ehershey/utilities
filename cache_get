#!/bin/bash
#
# Retrieve a url and cache it for an arbitrary period of time,
# avoiding duplicate requests if unexpired cached output exists.
# Usage: cache_get <lifetime> <url>
#

cachedir=$HOME/.generic_cache

TIMEOUT=2

if [ ! -d $cachedir ]
then
	mkdir $cachedir
fi

cache_life=$1
url=$2

if [ ! "$url" ]
then
	echo "usage: $0 <data_lifetime_seconds> <url>"
	exit 2
fi

if which md5 &>/dev/null
then
  md5=md5
elif which md5sum &> /dev/null
then
  md5=md5sum
else
  # don't use checksum for cache file
  #
  md5=cat
fi

cachefile=$cachedir/`echo "$url" | sed 's/[^0-9a-zA-Z_-]/_/g' | $md5`

if [ -e "$cachefile" ]
then
	# check if content has expired
	#
	#file_mtime=`stat -f %m $cachefile`
	file_mtime=`perl -MFile::stat -e 'print stat("'$cachefile'")->mtime'`

	current_time=`date +%s`
	if [ `expr $current_time - $file_mtime` -lt $cache_life ]
	then
		#echo cache hit >&2
    cat $cachefile
		exit
	fi
#else
    #echo "did not find file $cachefile" >&2
fi
#echo cache miss >&2
#lwp-request -m GET -t $TIMEOUT $url | tee $cachefile
curl --location --silent --connect-timeout $TIMEOUT --header 'User-Agent: Mozilla/5.0' $url | tee $cachefile
	
